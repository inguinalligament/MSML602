{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "OPWREI5RwG88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2dd3b83-2bff-4d4a-d2b0-4fb0ac6cc627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mAll done! ‚ú® üç∞ ‚ú®\u001b[0m\n",
            "\u001b[34m1 file \u001b[0mleft unchanged.\n"
          ]
        }
      ],
      "source": [
        "#BRADLEY SCOTT\n",
        "# this cell is just auto formatting so I can be lazy and still have pretty code\n",
        "#!pip install black[jupyter]\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount(\"/content/drive\")\n",
        "\n",
        "#!black /content/drive/MyDrive/'Colab Notebooks'/'DATA602_HW2.ipynb'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NOTE:\n",
        "# I used chatgpt quite a bit. This is maybe the second ML model I've ever trained.\n",
        "# I put a lot of effort into making sure I understood the process and changes being\n",
        "# suggested."
      ],
      "metadata": {
        "id": "JY1lWnSW44dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# I saved the data in my google drive so I could acccess it here\n",
        "file_path = \"/content/drive/My Drive/finefoods_training.txt\"\n",
        "\n",
        "# create an empty list to store the parsed data\n",
        "data = []\n",
        "\n",
        "# use 'ISO-8859-1' encoding instead of the default 'utf-8'\n",
        "# the utf-8 default encoding ran into issues\n",
        "with open(file_path, \"r\", encoding=\"ISO-8859-1\") as file:\n",
        "    review = {}\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            key, value = line.split(\": \", 1)\n",
        "            review[key] = value\n",
        "        else:\n",
        "            data.append(review)\n",
        "            review = {}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "# print(df.head())\n",
        "\n",
        "# wanted to see how many rows there is\n",
        "# num_rows = df.shape[0]\n",
        "# print(num_rows)\n",
        "\n",
        "# what's the average size of review/text - 401.366\n",
        "# lengths = df['review/text'].apply(len)\n",
        "# average_length = lengths.mean()\n",
        "# print(average_length)\n",
        "\n",
        "# what's the average size of review/summary - 22.8375\n",
        "# lengths = df['review/summary'].apply(len)\n",
        "# average_length = lengths.mean()\n",
        "# print(average_length)\n",
        "\n",
        "# NOTE: THIS WAS ME JUST INITIALLY LOOKING AT THE DATA AND WHAT NOT.\n",
        "# I IMPORT THE DATA AGAIN WHEN I CONSTRUCT THE ML MODELS\n",
        "# BECAUSE I WAS MAKING CHANGES TO THE WAY THAT I PROCESS THEM"
      ],
      "metadata": {
        "id": "JLK7QfUW2I7L"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# make vectors from review/summary and review/text\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\n",
        "summary_features = tfidf.fit_transform(df[\"review/summary\"])\n",
        "text_features = tfidf.fit_transform(df[\"review/text\"])\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "userId_encoded = encoder.fit_transform(df[\"review/userId\"])\n",
        "profileName_encoded = encoder.fit_transform(df[\"review/profileName\"])\n",
        "\n",
        "# combine the features and define the target variable\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "X = hstack(\n",
        "    [\n",
        "        summary_features,\n",
        "        text_features,\n",
        "        userId_encoded[:, None],\n",
        "        profileName_encoded[:, None],\n",
        "    ]\n",
        ")\n",
        "y = df[\"review/score\"]\n",
        "\n",
        "# train the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "# NOTE: This initial prediction is really only ok for category 5\n",
        "# which indicates an imbalance in the data. The next cell is the culimination of\n",
        "# me trying to find ways to make a better model that works well for more than just\n",
        "# category 5, but it required using other methods than Naive Bayes"
      ],
      "metadata": {
        "id": "8eZWeTcl3gfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad58551-bc26-465b-e0da-a689c4e643c0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.07      0.12      0.09        40\n",
            "         2.0       0.00      0.00      0.00        22\n",
            "         3.0       0.00      0.00      0.00        27\n",
            "         4.0       0.20      0.16      0.18        49\n",
            "         5.0       0.65      0.71      0.68       262\n",
            "\n",
            "    accuracy                           0.50       400\n",
            "   macro avg       0.18      0.20      0.19       400\n",
            "weighted avg       0.46      0.50      0.48       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading google pre-trained data to use in our ML model\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load Google's pre-trained Word2Vec model.\n",
        "w2v_model = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "SnIbidVvpFTr"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install spacy\n",
        "#!pip install imbalanced-learn\n",
        "#!pip install gensim\n",
        "\n",
        "# import needed modules\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# load spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "# spacy preprocessing\n",
        "def spacy_preprocess(text):\n",
        "    doc = nlp(text)\n",
        "    lemmatized = [\n",
        "        token.lemma_ for token in doc if not token.is_stop and not token.is_punct\n",
        "    ]\n",
        "    return \" \".join(lemmatized)\n",
        "\n",
        "\n",
        "# document vector creation\n",
        "def document_vector(word_list):\n",
        "    # remove out-of-vocabulary words\n",
        "    word_list = [word for word in word_list if word in w2v_model.key_to_index]\n",
        "    if len(word_list) == 0:\n",
        "        return np.zeros(300)  # Word2Vec uses 300\n",
        "    return np.mean(w2v_model[word_list], axis=0)\n",
        "\n",
        "\n",
        "# Read the data\n",
        "file_path = \"/content/drive/My Drive/finefoods_training.txt\"\n",
        "data = []\n",
        "\n",
        "with open(file_path, \"r\", encoding=\"ISO-8859-1\") as file:\n",
        "    review = {}\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            key, value = line.split(\": \", 1)\n",
        "            review[key] = value\n",
        "        else:\n",
        "            data.append(review)\n",
        "            review = {}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df[\"processed_summary\"] = df[\"review/summary\"].apply(spacy_preprocess)\n",
        "df[\"processed_text\"] = df[\"review/text\"].apply(spacy_preprocess)\n",
        "\n",
        "# create word2vec features\n",
        "df[\"w2v_summary\"] = df[\"processed_summary\"].apply(lambda x: document_vector(x.split()))\n",
        "df[\"w2v_text\"] = df[\"processed_text\"].apply(lambda x: document_vector(x.split()))\n",
        "\n",
        "# encode other categorical encoders\n",
        "encoder = LabelEncoder()\n",
        "userId_encoded = encoder.fit_transform(df[\"review/userId\"])\n",
        "profileName_encoded = encoder.fit_transform(df[\"review/profileName\"])\n",
        "\n",
        "# Combine all features\n",
        "X = np.hstack(\n",
        "    [\n",
        "        np.array(df[\"w2v_summary\"].tolist()),\n",
        "        np.array(df[\"w2v_text\"].tolist()),\n",
        "        userId_encoded[:, None],\n",
        "        profileName_encoded[:, None],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Target variable\n",
        "y = df[\"review/score\"].astype(\"category\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Apply SMOTE to the training data\n",
        "# SMOTE is used to try and correct for imbalances in the data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train the model with RandomForest\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# NOTE: Notice how the f1-score is better for the other classifications now.\n",
        "# overall, it's still not great with an accuracy of 0.645. It would likely require\n",
        "# a lot more tinkering for me to get the accuracy greatly improved."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMV8a84io4N8",
        "outputId": "16ee09c9-a158-45cb-f648-4390a8b7393e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.645\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.48      0.38      0.42        40\n",
            "         2.0       0.57      0.18      0.28        22\n",
            "         3.0       0.17      0.11      0.13        27\n",
            "         4.0       0.08      0.04      0.05        49\n",
            "         5.0       0.74      0.89      0.81       262\n",
            "\n",
            "    accuracy                           0.65       400\n",
            "   macro avg       0.41      0.32      0.34       400\n",
            "weighted avg       0.58      0.65      0.60       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional Task\n",
        "# predict the review/helpfulness form the rest of the information\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import spacy\n",
        "from scipy.sparse import hstack\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# function to preprocess text with spacy\n",
        "def spacy_preprocess(text, nlp_model):\n",
        "    doc = nlp_model(text)\n",
        "    return \" \".join(\n",
        "        [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    )\n",
        "\n",
        "\n",
        "# function to convert helpfulness to ratio\n",
        "# I don't think predicting the helpfulness numbers\n",
        "# themselves (so like 10/10 or 7/8) would be possible to get a good prediction of\n",
        "# and I don't think it's as useful as the actual helpfulness ratio\n",
        "def helpfulness_ratio(helpfulness):\n",
        "    num, denom = map(int, helpfulness.split(\"/\"))\n",
        "    if denom == 0:\n",
        "        return np.nan  # Return NaN for 0 denominators\n",
        "    return num / denom\n",
        "\n",
        "\n",
        "# load the spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# load the dataset\n",
        "file_path = \"/content/drive/My Drive/finefoods_training.txt\"\n",
        "data = []\n",
        "\n",
        "with open(file_path, \"r\", encoding=\"ISO-8859-1\") as file:\n",
        "    review = {}\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            key, value = line.split(\": \", 1)\n",
        "            review[key] = value\n",
        "        else:\n",
        "            data.append(review)\n",
        "            review = {}\n",
        "\n",
        "# create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# convert time to a datetime object and then to a numerical feature\n",
        "df[\"review/time\"] = pd.to_datetime(df[\"review/time\"].astype(int), unit=\"s\")\n",
        "latest_review_time = df[\"review/time\"].max()\n",
        "df[\"review_time_diff\"] = (latest_review_time - df[\"review/time\"]).dt.days\n",
        "\n",
        "# preprocess text data\n",
        "df[\"processed_summary\"] = df[\"review/summary\"].apply(lambda x: spacy_preprocess(x, nlp))\n",
        "df[\"processed_text\"] = df[\"review/text\"].apply(lambda x: spacy_preprocess(x, nlp))\n",
        "\n",
        "# convert helpfulness to ratio.\n",
        "df[\"helpfulness_ratio\"] = df[\"review/helpfulness\"].apply(helpfulness_ratio)\n",
        "\n",
        "# drop rows with NaN helpfulness_ratio\n",
        "df.dropna(subset=[\"helpfulness_ratio\"], inplace=True)\n",
        "\n",
        "# vectorize text data\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "summary_features = tfidf.fit_transform(df[\"processed_summary\"])\n",
        "text_features = tfidf.fit_transform(df[\"processed_text\"])\n",
        "\n",
        "# encode categorical features and scale numerical features\n",
        "encoder = LabelEncoder()\n",
        "userId_encoded = encoder.fit_transform(df[\"review/userId\"])\n",
        "profileName_encoded = encoder.fit_transform(df[\"review/profileName\"])\n",
        "productId_encoded = encoder.fit_transform(df[\"product/productId\"])\n",
        "score_encoded = encoder.fit_transform(df[\"review/score\"])\n",
        "scaler = StandardScaler()\n",
        "# adjust for the time scale (mentioned in project description)\n",
        "review_time_diff_scaled = scaler.fit_transform(df[[\"review_time_diff\"]])\n",
        "\n",
        "# combine the features\n",
        "X = hstack(\n",
        "    [\n",
        "        summary_features,\n",
        "        text_features,\n",
        "        userId_encoded[:, None],\n",
        "        profileName_encoded[:, None],\n",
        "        productId_encoded[:, None],\n",
        "        score_encoded[:, None],\n",
        "        review_time_diff_scaled,\n",
        "    ]\n",
        ")\n",
        "y = df[\"helpfulness_ratio\"]\n",
        "\n",
        "# split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# train the model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions and check MSE and R squared\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "mse, r2\n",
        "\n",
        "# NOTE: THe MSE and R squared values are not great. We changed the review/helpfulness\n",
        "# to a ratio so it's between 0 and 1. An MSE of 0.1 is pretty high for this range.\n",
        "# The R squared value means that we can only explain about 20% of the variance\n",
        "# from the model. I didn't think this would be an easy thing to model since\n",
        "# a lot of the reviews are going to have 0/0 for review/helpfulness without\n",
        "# any obvious reason why"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwqcKypGqClr",
        "outputId": "85e2913d-5e6e-4020-b45a-9dfa71c76143"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.10315733619839082, 0.2012137219861615)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    }
  ]
}